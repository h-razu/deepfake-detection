{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hXNXXfaR8rU2"
      ],
      "mount_file_id": "1vPam2g4-7g4oo4S_VQbZpxAVKCfm9BHt",
      "authorship_tag": "ABX9TyMb7nOgtAEdpYlfteNLJT4j"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Environment"
      ],
      "metadata": {
        "id": "ljb3JehSeQd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Required Library"
      ],
      "metadata": {
        "id": "PCwMh9PxeWgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import yaml\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms.functional import to_pil_image"
      ],
      "metadata": {
        "id": "__KiAqEK7ZQ-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the Version of Library"
      ],
      "metadata": {
        "id": "K00PmXnnelDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Platform:\", sys.platform)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"==============================\")\n",
        "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"PyYAML version:\", yaml.__version__)\n",
        "print(\"PIL version : \", PIL.__version__)\n",
        "print(\"CV2 version : \", cv2.__version__)\n",
        "print(\"torch version : \", torch.__version__)\n",
        "print(\"torchvision version : \", torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DGznLhnejx_",
        "outputId": "41a8a97e-a5da-4d0e-f99e-0d5e2a5027ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: linux\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "==============================\n",
            "matplotlib version: 3.10.0\n",
            "pandas version: 2.2.2\n",
            "PyYAML version: 6.0.2\n",
            "PIL version :  11.2.1\n",
            "CV2 version :  4.11.0\n",
            "torch version :  2.6.0+cu124\n",
            "torchvision version :  0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check GPU Availability"
      ],
      "metadata": {
        "id": "hIYuFp25d2Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aH2HpTtd73s",
        "outputId": "33fd5944-43d8-495f-e8b1-1bcad8a9cacb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting Google Drive for Accessing Datasets"
      ],
      "metadata": {
        "id": "C1JL_jlqfIzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHZ56fwjfKTx",
        "outputId": "8246bcca-bf0e-48a5-b0d9-dfa066a8a6ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/dataset/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic64IwrHfeIT",
        "outputId": "0f62dbe5-bb25-402a-cbb2-9484961948a9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsdN8UlofrOg",
        "outputId": "3a82abe0-e977-440f-ac64-36d52ca7afd0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (174 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree faceforensics --filelimit=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-lxRBJfy0J",
        "outputId": "03f31991-249d-4b8e-db37-8b77fb0e878f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mfaceforensics\u001b[0m\n",
            "├── \u001b[01;34mmanipulated_sequences\u001b[0m\n",
            "│   ├── \u001b[01;34mDeepfakes\u001b[0m\n",
            "│   │   └── \u001b[01;34mc23\u001b[0m\n",
            "│   │       └── \u001b[01;34mvideos\u001b[0m  [100 entries exceeds filelimit, not opening dir]\n",
            "│   ├── \u001b[01;34mFace2Face\u001b[0m\n",
            "│   │   └── \u001b[01;34mc23\u001b[0m\n",
            "│   │       └── \u001b[01;34mvideos\u001b[0m  [100 entries exceeds filelimit, not opening dir]\n",
            "│   ├── \u001b[01;34mFaceSwap\u001b[0m\n",
            "│   │   └── \u001b[01;34mc23\u001b[0m\n",
            "│   │       └── \u001b[01;34mvideos\u001b[0m  [100 entries exceeds filelimit, not opening dir]\n",
            "│   └── \u001b[01;34mNeuralTextures\u001b[0m\n",
            "│       └── \u001b[01;34mc23\u001b[0m\n",
            "│           └── \u001b[01;34mvideos\u001b[0m  [100 entries exceeds filelimit, not opening dir]\n",
            "└── \u001b[01;34moriginal_sequences\u001b[0m\n",
            "    └── \u001b[01;34myoutube\u001b[0m\n",
            "        └── \u001b[01;34mc23\u001b[0m\n",
            "            └── \u001b[01;34mvideos\u001b[0m  [100 entries exceeds filelimit, not opening dir]\n",
            "\n",
            "17 directories, 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scripts"
      ],
      "metadata": {
        "id": "VflxP49O6l7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download FaceForensics Dataset"
      ],
      "metadata": {
        "id": "hXNXXfaR8rU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile faceforensics_download.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\" Downloads FaceForensics++ and Deep Fake Detection public data release\n",
        "Example usage:\n",
        "    see -h or https://github.com/ondyari/FaceForensics\n",
        "\"\"\"\n",
        "# -*- coding: utf-8 -*-\n",
        "import argparse\n",
        "import os\n",
        "import urllib\n",
        "import urllib.request\n",
        "import tempfile\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "# URLs and filenames\n",
        "FILELIST_URL = 'misc/filelist.json'\n",
        "DEEPFEAKES_DETECTION_URL = 'misc/deepfake_detection_filenames.json'\n",
        "DEEPFAKES_MODEL_NAMES = ['decoder_A.h5', 'decoder_B.h5', 'encoder.h5',]\n",
        "\n",
        "# Parameters\n",
        "DATASETS = {\n",
        "    'original_youtube_videos': 'misc/downloaded_youtube_videos.zip',\n",
        "    'original_youtube_videos_info': 'misc/downloaded_youtube_videos_info.zip',\n",
        "    'original': 'original_sequences/youtube',\n",
        "    'DeepFakeDetection_original': 'original_sequences/actors',\n",
        "    'Deepfakes': 'manipulated_sequences/Deepfakes',\n",
        "    'DeepFakeDetection': 'manipulated_sequences/DeepFakeDetection',\n",
        "    'Face2Face': 'manipulated_sequences/Face2Face',\n",
        "    'FaceShifter': 'manipulated_sequences/FaceShifter',\n",
        "    'FaceSwap': 'manipulated_sequences/FaceSwap',\n",
        "    'NeuralTextures': 'manipulated_sequences/NeuralTextures'\n",
        "    }\n",
        "ALL_DATASETS = ['original', 'DeepFakeDetection_original', 'Deepfakes',\n",
        "                'DeepFakeDetection', 'Face2Face', 'FaceShifter', 'FaceSwap',\n",
        "                'NeuralTextures']\n",
        "COMPRESSION = ['raw', 'c23', 'c40']\n",
        "TYPE = ['videos', 'masks', 'models']\n",
        "SERVERS = ['EU', 'EU2', 'CA']\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Downloads FaceForensics v2 public data release.',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "    parser.add_argument('output_path', type=str, help='Output directory.')\n",
        "    parser.add_argument('-d', '--dataset', type=str, default='all',\n",
        "                        help='Which dataset to download, either pristine or '\n",
        "                             'manipulated data or the downloaded youtube '\n",
        "                             'videos.',\n",
        "                        choices=list(DATASETS.keys()) + ['all']\n",
        "                        )\n",
        "    parser.add_argument('-c', '--compression', type=str, default='raw',\n",
        "                        help='Which compression degree. All videos '\n",
        "                             'have been generated with h264 with a varying '\n",
        "                             'codec. Raw (c0) videos are lossless compressed.',\n",
        "                        choices=COMPRESSION\n",
        "                        )\n",
        "    parser.add_argument('-t', '--type', type=str, default='videos',\n",
        "                        help='Which file type, i.e. videos, masks, for our '\n",
        "                             'manipulation methods, models, for Deepfakes.',\n",
        "                        choices=TYPE\n",
        "                        )\n",
        "    parser.add_argument('-n', '--num_videos', type=int, default=None,\n",
        "                        help='Select a number of videos number to '\n",
        "                             \"download if you don't want to download the full\"\n",
        "                             ' dataset.')\n",
        "    parser.add_argument('--server', type=str, default='EU',\n",
        "                        help='Server to download the data from. If you '\n",
        "                             'encounter a slow download speed, consider '\n",
        "                             'changing the server.',\n",
        "                        choices=SERVERS\n",
        "                        )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # URLs\n",
        "    server = args.server\n",
        "    if server == 'EU':\n",
        "        server_url = 'http://canis.vc.in.tum.de:8100/'\n",
        "    elif server == 'EU2':\n",
        "        server_url = 'http://kaldir.vc.in.tum.de/faceforensics/'\n",
        "    elif server == 'CA':\n",
        "        server_url = 'http://falas.cmpt.sfu.ca:8100/'\n",
        "    else:\n",
        "        raise Exception('Wrong server name. Choices: {}'.format(str(SERVERS)))\n",
        "    args.tos_url = server_url + 'webpage/FaceForensics_TOS.pdf'\n",
        "    args.base_url = server_url + 'v3/'\n",
        "    args.deepfakes_model_url = server_url + 'v3/manipulated_sequences/' + \\\n",
        "                               'Deepfakes/models/'\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def download_files(filenames, base_url, output_path, report_progress=True):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    if report_progress:\n",
        "        filenames = tqdm(filenames)\n",
        "    for filename in filenames:\n",
        "        download_file(base_url + filename, join(output_path, filename))\n",
        "\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = int(progress_size / (1024 * duration))\n",
        "    percent = int(count * block_size * 100 / total_size)\n",
        "    sys.stdout.write(\"\\rProgress: %d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
        "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def download_file(url, out_file, report_progress=False):\n",
        "    out_dir = os.path.dirname(out_file)\n",
        "    if not os.path.isfile(out_file):\n",
        "        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n",
        "        f = os.fdopen(fh, 'w')\n",
        "        f.close()\n",
        "        if report_progress:\n",
        "            urllib.request.urlretrieve(url, out_file_tmp,\n",
        "                                       reporthook=reporthook)\n",
        "        else:\n",
        "            urllib.request.urlretrieve(url, out_file_tmp)\n",
        "        os.rename(out_file_tmp, out_file)\n",
        "    else:\n",
        "        tqdm.write('WARNING: skipping download of existing file ' + out_file)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # TOS\n",
        "    print('By pressing any key to continue you confirm that you have agreed '\\\n",
        "          'to the FaceForensics terms of use as described at:')\n",
        "    print(args.tos_url)\n",
        "    print('***')\n",
        "    print('Press any key to continue, or CTRL-C to exit.')\n",
        "    _ = input('')\n",
        "\n",
        "    # Extract arguments\n",
        "    c_datasets = [args.dataset] if args.dataset != 'all' else ALL_DATASETS\n",
        "    c_type = args.type\n",
        "    c_compression = args.compression\n",
        "    num_videos = args.num_videos\n",
        "    output_path = args.output_path\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Check for special dataset cases\n",
        "    for dataset in c_datasets:\n",
        "        dataset_path = DATASETS[dataset]\n",
        "        # Special cases\n",
        "        if 'original_youtube_videos' in dataset:\n",
        "            # Here we download the original youtube videos zip file\n",
        "            print('Downloading original youtube videos.')\n",
        "            if not 'info' in dataset_path:\n",
        "                print('Please be patient, this may take a while (~40gb)')\n",
        "                suffix = ''\n",
        "            else:\n",
        "                suffix = 'info'\n",
        "                download_file(args.base_url + '/' + dataset_path,\n",
        "                          out_file=join(output_path,\n",
        "                                        'downloaded_videos{}.zip'.format(\n",
        "                                            suffix)),\n",
        "                          report_progress=True)\n",
        "            return\n",
        "\n",
        "        # Else: regular datasets\n",
        "        print('Downloading {} of dataset \"{}\"'.format(\n",
        "            c_type, dataset_path\n",
        "        ))\n",
        "\n",
        "        # Get filelists and video lenghts list from server\n",
        "        if 'DeepFakeDetection' in dataset_path or 'actors' in dataset_path:\n",
        "            filepaths = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                DEEPFEAKES_DETECTION_URL).read().decode(\"utf-8\"))\n",
        "            if 'actors' in dataset_path:\n",
        "                filelist = filepaths['actors']\n",
        "            else:\n",
        "                filelist = filepaths['DeepFakesDetection']\n",
        "        elif 'original' in dataset_path:\n",
        "            # Load filelist from server\n",
        "            file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                FILELIST_URL).read().decode(\"utf-8\"))\n",
        "            filelist = []\n",
        "            for pair in file_pairs:\n",
        "                filelist += pair\n",
        "        else:\n",
        "            # Load filelist from server\n",
        "            file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                FILELIST_URL).read().decode(\"utf-8\"))\n",
        "            # Get filelist\n",
        "            filelist = []\n",
        "            for pair in file_pairs:\n",
        "                filelist.append('_'.join(pair))\n",
        "                if c_type != 'models':\n",
        "                    filelist.append('_'.join(pair[::-1]))\n",
        "\n",
        "        # Maybe limit number of videos for download\n",
        "        if num_videos is not None and num_videos > 0:\n",
        "            print('Downloading the first {} videos'.format(num_videos))\n",
        "            filelist = filelist[:num_videos]\n",
        "\n",
        "        # Server and local paths\n",
        "            dataset_videos_url = args.base_url + '{}/{}/{}/'.format(\n",
        "            dataset_path, c_compression, c_type)\n",
        "            dataset_mask_url = args.base_url + '{}/{}/videos/'.format(\n",
        "            dataset_path, 'masks', c_type)\n",
        "\n",
        "        if c_type == 'videos':\n",
        "            dataset_output_path = join(output_path, dataset_path, c_compression,\n",
        "                                       c_type)\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "            filelist = [filename + '.mp4' for filename in filelist]\n",
        "            download_files(filelist, dataset_videos_url, dataset_output_path)\n",
        "\n",
        "        elif c_type == 'masks':\n",
        "            dataset_output_path = join(output_path, dataset_path, c_type,\n",
        "                                       'videos')\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "            if 'original' in dataset:\n",
        "                if args.dataset != 'all':\n",
        "                    print('Only videos available for original data. Aborting.')\n",
        "                    return\n",
        "                else:\n",
        "                    print('Only videos available for original data. '\n",
        "                          'Skipping original.\\n')\n",
        "                    continue\n",
        "            if 'FaceShifter' in dataset:\n",
        "                print('Masks not available for FaceShifter. Aborting.')\n",
        "                return\n",
        "            filelist = [filename + '.mp4' for filename in filelist]\n",
        "            download_files(filelist, dataset_mask_url, dataset_output_path)\n",
        "\n",
        "        # Else: models for deepfakes\n",
        "        else:\n",
        "            if dataset != 'Deepfakes' and c_type == 'models':\n",
        "                print('Models only available for Deepfakes. Aborting')\n",
        "                return\n",
        "            dataset_output_path = join(output_path, dataset_path, c_type)\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "\n",
        "            # Get Deepfakes models\n",
        "            for folder in tqdm(filelist):\n",
        "                folder_filelist = DEEPFAKES_MODEL_NAMES\n",
        "\n",
        "                # Folder paths\n",
        "                folder_base_url = args.deepfakes_model_url + folder + '/'\n",
        "                folder_dataset_output_path = join(dataset_output_path,\n",
        "                                                  folder)\n",
        "                download_files(folder_filelist, folder_base_url,\n",
        "                               folder_dataset_output_path,\n",
        "                               report_progress=False)   # already done\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugkItCbE6p8K",
        "outputId": "70bf3203-a902-4273-a55c-189b147d03b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting faceforensics_download.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sns-Tm08OqD",
        "outputId": "983b0a96-9d30-4755-fb05-b80e9169ef51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/drive/MyDrive/datasets/faceforensics'"
      ],
      "metadata": {
        "id": "f21ug6R98gLX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "# Remove existing dataset folder (if any)\n",
        "if os.path.exists(output_path):\n",
        "    print(f\"Deleting existing folder: {output_path}\")\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "# Recreate a clean folder\n",
        "os.makedirs(output_path, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfweUsE6-Mah",
        "outputId": "ae0204a1-d174-44b0-f589-22ad850b3044"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting existing folder: /content/drive/MyDrive/datasets/faceforensics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python faceforensics_download.py --output_path /content/faceforensics --dataset Deepfakes --compression c23 --type videos --num_videos 30 --server EU"
      ],
      "metadata": {
        "id": "qwxdNTez7gfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download `original` dataset\n",
        "!python faceforensics_download.py \"/content/drive/My Drive/dataset/faceforensics\" \\\n",
        "    -d original \\\n",
        "    -c c23 \\\n",
        "    -t videos \\\n",
        "    -n 100 \\\n",
        "    --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59DKwtwJAY3T",
        "outputId": "1099090b-6410-4b1f-8ae3-94a53793e3eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n",
            "http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n",
            "***\n",
            "Press any key to continue, or CTRL-C to exit.\n",
            "\n",
            "Downloading videos of dataset \"original_sequences/youtube\"\n",
            "Downloading the first 30 videos\n",
            "Output path: /content/drive/My Drive/dataset/faceforensics/original_sequences/youtube/c23/videos\n",
            "100% 30/30 [52:32<00:00, 105.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download `Face2Face` dataset\n",
        "!python faceforensics_download.py \"/content/drive/My Drive/dataset/faceforensics\" \\\n",
        "    -d Face2Face \\\n",
        "    -c c23 \\\n",
        "    -t videos \\\n",
        "    -n 1 \\\n",
        "    --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvZ90--bAfV2",
        "outputId": "67ed612e-75e1-4c56-8df3-14b9ed0cb623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n",
            "http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n",
            "***\n",
            "Press any key to continue, or CTRL-C to exit.\n",
            "\n",
            "Downloading videos of dataset \"manipulated_sequences/Face2Face\"\n",
            "Downloading the first 1 videos\n",
            "Output path: /content/drive/My Drive/dataset/faceforensics/manipulated_sequences/Face2Face/c23/videos\n",
            "100% 1/1 [01:55<00:00, 115.16s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download `FaceSwap` dataset\n",
        "!python faceforensics_download.py \"/content/drive/My Drive/dataset/faceforensics\" \\\n",
        "    -d FaceSwap \\\n",
        "    -c c23 \\\n",
        "    -t videos \\\n",
        "    -n 1 \\\n",
        "    --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCEbjb3_Agpa",
        "outputId": "a70c1a66-e3ac-42b1-b9cf-ac1181347a92"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n",
            "http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n",
            "***\n",
            "Press any key to continue, or CTRL-C to exit.\n",
            "\n",
            "Downloading videos of dataset \"manipulated_sequences/FaceSwap\"\n",
            "Downloading the first 1 videos\n",
            "Output path: /content/drive/My Drive/dataset/faceforensics/manipulated_sequences/FaceSwap/c23/videos\n",
            "100% 1/1 [01:26<00:00, 86.16s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download `Deepfakes` dataset\n",
        "!python faceforensics_download.py \"/content/drive/My Drive/dataset/faceforensics\" \\\n",
        "    -d Deepfakes \\\n",
        "    -c c23 \\\n",
        "    -t videos \\\n",
        "    -n 1 \\\n",
        "    --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WtEB0xsAhqj",
        "outputId": "434b4002-d2eb-4679-b175-b70815d1002b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n",
            "http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n",
            "***\n",
            "Press any key to continue, or CTRL-C to exit.\n",
            "\n",
            "Downloading videos of dataset \"manipulated_sequences/Deepfakes\"\n",
            "Downloading the first 1 videos\n",
            "Output path: /content/drive/My Drive/dataset/faceforensics/manipulated_sequences/Deepfakes/c23/videos\n",
            "100% 1/1 [01:51<00:00, 111.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download `NeuralTextures` dataset\n",
        "!python faceforensics_download.py \"/content/drive/My Drive/dataset/faceforensics\" \\\n",
        "    -d NeuralTextures \\\n",
        "    -c c23 \\\n",
        "    -t videos \\\n",
        "    -n 1 \\\n",
        "    --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPvUAqK9AjNd",
        "outputId": "f04b5697-a966-45ab-a70d-eb8917021012"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n",
            "http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n",
            "***\n",
            "Press any key to continue, or CTRL-C to exit.\n",
            "\n",
            "Downloading videos of dataset \"manipulated_sequences/NeuralTextures\"\n",
            "Downloading the first 1 videos\n",
            "Output path: /content/drive/My Drive/dataset/faceforensics/manipulated_sequences/NeuralTextures/c23/videos\n",
            "100% 1/1 [01:48<00:00, 108.60s/it]\n"
          ]
        }
      ]
    }
  ]
}